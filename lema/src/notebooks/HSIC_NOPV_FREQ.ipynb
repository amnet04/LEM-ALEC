{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CÃ¡lculos de HSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('/src')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas\n",
    "import re\n",
    "from HSIC.HSIC import HSIC_no_pval\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_ORIGIN = \"/src/data/procesados/clean/TWITTER_COLOMBIA_FREQ_CLEAN.csv\"\n",
    "CITIES_ORIGIN = \"/src/data/procesados/geo/filter_cities_coordinates.csv\"\n",
    "DESTINO = \"/src/data/procesados/HSIC/HSIC_RANK_NORM_SIN_PVALUE_FREQ.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escribir_cabezote_archivo(destino, fieldnames):\n",
    "    with open(destino, 'w') as csvfile:\n",
    "        writer = fieldnames.to_csv(csvfile,sep=\"\\t\",header=fieldnames.columns,decimal=\",\")\n",
    "        \n",
    "def escribir_datos_archivo(destino, values):\n",
    "    with open(destino, 'a') as csvfile:\n",
    "        writer = values.to_csv(csvfile,sep=\"\\t\",header=False,decimal=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_no_pvalue_to_file(words_file, cities_file, destino):\n",
    "    words = pandas.read_csv(\n",
    "        words_file, \n",
    "        encoding='utf-8',\n",
    "        chunksize=1,\n",
    "        index_col=0,\n",
    "        skiprows=[1],\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"'\n",
    "    )\n",
    "    cities = pandas.read_csv(\n",
    "        cities_file, \n",
    "        encoding='utf-8',\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"',\n",
    "        usecols=['ciudad', 'Latitud','Longitud'],\n",
    "        index_col =0\n",
    "    ).transpose()\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for chunk  in words:\n",
    "        start_time = time.time()\n",
    "        data = pandas.concat([cities,chunk]).transpose()\n",
    "        word_vector = data.iloc[0:, 2:3].as_matrix()\n",
    "        #Linealizar\n",
    "        linear_word_vector = np.log(word_vector+1)\n",
    "        cities_vector = data.iloc[0:, 0:2].as_matrix()\n",
    "        hsic=HSIC_no_pval(cities_vector,word_vector, kernelX=\"Gaussian\", kernelY=\"Gaussian\")\n",
    "        hsic_lin=HSIC_no_pval(cities_vector,linear_word_vector, kernelX=\"Gaussian\", kernelY=\"Gaussian\")\n",
    "        basic_header = pandas.DataFrame({\n",
    "            'HSIC':pandas.Series([], dtype='float'),\n",
    "            'HSIC_LIN':pandas.Series([], dtype='float')\n",
    "        })\n",
    "        basic_header.loc['{}'.format(chunk.index[0]),'HSIC']=hsic\n",
    "        basic_header.loc['{}'.format(chunk.index[0]),'HSIC_LIN']=hsic_lin\n",
    "        if count == 0:\n",
    "            escribir_cabezote_archivo(destino, basic_header)\n",
    "        else:\n",
    "            escribir_datos_archivo(destino, basic_header)\n",
    "        count = count + 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print ('\\rDuracion:{}\\t procesados:{}\\t palabra:{}\\t'.format(elapsed_time,count, chunk.index.values[0]),end='\\t')\n",
    "    \n",
    "    return(hsic, hsic_lin)\n",
    "            \n",
    "    \n",
    "    \"\"\"print('\\n \\n Frito el pollo')\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con frecuencias absolutas \n",
    "\n",
    "WORDS_ORIGIN = \"/src/data/procesados/clean/TWITTER_COLOMBIA_FREQ_CLEAN.csv\"\n",
    "CITIES_ORIGIN = \"/src/data/procesados/geo/filter_cities_coordinates.csv\"\n",
    "DESTINO = \"/src/data/procesados/HSIC/HSIC_SIN_PVALUE_FREQ.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duracion:0.06232762336730957\t procesados:130464\t palabra:casaenelagua\t\t\t\t\t\t\t\tbas\t\tlicaigual\t\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duracion:0.057520389556884766\t procesados:413040\t palabra:tardata\t\t\t\tntoss\t\t\t\ty\t\tte\t\t\tiaz\t\ttiaportiypensarasenmiaunqueesteslejos\t\tu\t\t\t\thablandohermosofiebreanochededicarimagenes\t\tatarara\t\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.1257991320446221e-05, 7.1257991320446221e-05)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsic_no_pvalue_to_file(WORDS_ORIGIN, CITIES_ORIGIN, DESTINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con frecuencias realtivas \n",
    "\n",
    "WORDS_ORIGIN = \"/src/data/procesados/clean/TWITTER_COLOMBIA_RELATIVE_FREQ_CLEAN.csv\"\n",
    "DESTINO = \"/src/data/procesados/HSIC/HSIC_SIN_PVALUE_REL_FREQ.csv\"\n",
    "def hsic_no_pvalue_to_file(words_file, cities_file, destino):\n",
    "    words = pandas.read_csv(\n",
    "        words_file, \n",
    "        encoding='utf-8',\n",
    "        chunksize=1,\n",
    "        index_col=0,\n",
    "        skiprows=[1],\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"'\n",
    "    )\n",
    "    cities = pandas.read_csv(\n",
    "        cities_file, \n",
    "        encoding='utf-8',\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"',\n",
    "        usecols=['ciudad', 'Latitud','Longitud'],\n",
    "        index_col =0\n",
    "    ).transpose()\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for chunk  in words:\n",
    "        start_time = time.time()\n",
    "        data = pandas.concat([cities,chunk]).transpose()\n",
    "        word_vector = data.iloc[0:, 2:3].as_matrix()\n",
    "        #Linealizar\n",
    "        linear_word_vector = np.log(word_vector*100000000)\n",
    "        cities_vector = data.iloc[0:, 0:2].as_matrix()\n",
    "        hsic=HSIC_no_pval(cities_vector,word_vector, kernelX=\"Gaussian\", kernelY=\"Gaussian\")\n",
    "        hsic_lin=HSIC_no_pval(cities_vector,linear_word_vector, kernelX=\"Gaussian\", kernelY=\"Gaussian\")\n",
    "        basic_header = pandas.DataFrame({\n",
    "            'HSIC':pandas.Series([], dtype='float'),\n",
    "            'HSIC_LIN':pandas.Series([], dtype='float')\n",
    "        })\n",
    "        basic_header.loc['{}'.format(chunk.index[0]),'HSIC']=hsic\n",
    "        basic_header.loc['{}'.format(chunk.index[0]),'HSIC_LIN']=hsic_lin\n",
    "        if count == 0:\n",
    "            escribir_cabezote_archivo(destino, basic_header)\n",
    "        else:\n",
    "            escribir_datos_archivo(destino, basic_header)\n",
    "        count = count + 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print ('\\rDuracion:{}\\t procesados:{}\\t palabra:{}\\t'.format(elapsed_time,count, chunk.index.values[0]),end='\\t')\n",
    "    \n",
    "    return(hsic, hsic_lin)\n",
    "            \n",
    "    \n",
    "    \"\"\"print('\\n \\n Frito el pollo')\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duracion:0.056249141693115234\t procesados:130464\t palabra:casaenelagua\t\t\t\t\t\t\t\tas\t\tlicaigual\t\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duracion:0.05717825889587402\t procesados:413040\t palabra:tardata\t\t\t\tentoss\t\t\t\t\t\tte\t\t\t\t\t\tz\t\tiaportiypensarasenmiaunqueesteslejos\t\ttu\t\t\thablandohermosofiebreanochededicarimagenes\t\ttatarara\t\t\t\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.0840356939155631e-05, 0.0)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsic_no_pvalue_to_file(WORDS_ORIGIN, CITIES_ORIGIN, DESTINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con ranks \n",
    "\n",
    "WORDS_ORIGIN = \"/src/data/procesados/clean/TWITTER_COLOMBIA_RANK.csv\"\n",
    "DESTINO = \"/src/data/procesados/HSIC/HSIC_SIN_PVALUE_RANK.csv\"\n",
    "\n",
    "def hsic_no_pvalue_to_file(words_file, cities_file, destino):\n",
    "    words = pandas.read_csv(\n",
    "        words_file, \n",
    "        encoding='utf-8',\n",
    "        chunksize=1,\n",
    "        index_col=0,\n",
    "        skiprows=[1],\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"'\n",
    "    )\n",
    "    cities = pandas.read_csv(\n",
    "        cities_file, \n",
    "        encoding='utf-8',\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"',\n",
    "        usecols=['ciudad', 'Latitud','Longitud'],\n",
    "        index_col =0\n",
    "    ).transpose()\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for chunk  in words:\n",
    "        start_time = time.time()\n",
    "        data = pandas.concat([cities,chunk]).transpose()\n",
    "        word_vector = data.iloc[0:, 2:3].as_matrix()\n",
    "        #Linealizar\n",
    "        linear_word_vector = np.log(word_vector)\n",
    "        cities_vector = data.iloc[0:, 0:2].as_matrix()\n",
    "        hsic=HSIC_no_pval(cities_vector,word_vector, kernelX=\"Gaussian\", kernelY=\"Gaussian\")\n",
    "        hsic_lin=HSIC_no_pval(cities_vector,linear_word_vector, kernelX=\"Gaussian\", kernelY=\"Gaussian\")\n",
    "        basic_header = pandas.DataFrame({\n",
    "            'HSIC':pandas.Series([], dtype='float'),\n",
    "            'HSIC_LIN':pandas.Series([], dtype='float')\n",
    "        })\n",
    "        basic_header.loc['{}'.format(chunk.index[0]),'HSIC']=hsic\n",
    "        basic_header.loc['{}'.format(chunk.index[0]),'HSIC_LIN']=hsic_lin\n",
    "        if count == 0:\n",
    "            escribir_cabezote_archivo(destino, basic_header)\n",
    "        else:\n",
    "            escribir_datos_archivo(destino, basic_header)\n",
    "        count = count + 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print ('\\rDuracion:{}\\t procesados:{}\\t palabra:{}\\t'.format(elapsed_time,count, chunk.index.values[0]),end='\\t')\n",
    "    \n",
    "    return(hsic, hsic_lin)\n",
    "            \n",
    "    \n",
    "    \"\"\"print('\\n \\n Frito el pollo')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duracion:0.0830388069152832\t procesados:7310\t palabra:ocupa\t\t\t\timiento\t\t\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-89ebfe618771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhsic_no_pvalue_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORDS_ORIGIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCITIES_ORIGIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDESTINO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-193-a616b155b157>\u001b[0m in \u001b[0;36mhsic_no_pvalue_to_file\u001b[0;34m(words_file, cities_file, destino)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mcities_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mhsic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHSIC_no_pval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcities_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Gaussian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Gaussian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mhsic_lin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHSIC_no_pval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcities_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinear_word_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Gaussian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Gaussian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         basic_header = pandas.DataFrame({\n\u001b[1;32m     40\u001b[0m             \u001b[0;34m'HSIC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/HSIC/HSIC.py\u001b[0m in \u001b[0;36mHSIC_no_pval\u001b[0;34m(X, Y, N_samp, kernelX, kernelY, eta, sigmaX, sigmaY, p_method, return_boots)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0msigmaY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSigmaGaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msigmaY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msigmaY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_rankA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincompleteCholeskyKernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmaX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_rankB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincompleteCholeskyKernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmaY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/HSIC/HSIC.py\u001b[0m in \u001b[0;36mincompleteCholeskyKernel\u001b[0;34m(X, maxrank, kernel, sigma, eta)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnu_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hsic_no_pvalue_to_file(WORDS_ORIGIN, CITIES_ORIGIN, DESTINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con ranks normalizados\n",
    "\n",
    "# Corregir #\n",
    "\n",
    "WORDS_ORIGIN = \"/src/data/procesados/clean/TWITTER_COLOMBIA_RANK_NORM.csv\"\n",
    "DESTINO = \"/src/data/procesados/HSIC/HSIC_SIN_PVALUE_RANK_NORM.csv\"\n",
    "\n",
    "def hsic_no_pvalue_to_file(words_file, cities_file, destino):\n",
    "    words = pandas.read_csv(\n",
    "        words_file, \n",
    "        encoding='utf-8',\n",
    "        chunksize=1,\n",
    "        index_col=0,\n",
    "        skiprows=[1],\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"'\n",
    "    )\n",
    "    cities = pandas.read_csv(\n",
    "        cities_file, \n",
    "        encoding='utf-8',\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"',\n",
    "        usecols=['ciudad', 'Latitud','Longitud'],\n",
    "        index_col =0\n",
    "    ).transpose()\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for chunk  in words:\n",
    "        start_time = time.time()\n",
    "        data = pandas.concat([cities,chunk]).transpose()\n",
    "        word_vector = data.iloc[0:, 2:3].as_matrix()\n",
    "        #Linealizar\n",
    "        linear_word_vector = np.log(word_vector*100000000)\n",
    "        cities_vector = data.iloc[0:, 0:2].as_matrix()\n",
    "        hsic=HSIC_no_pval(cities_vector,word_vector, kernelX=\"Gaussian\", kernelY=\"Gaussian\")\n",
    "        hsic_lin=HSIC_no_pval(cities_vector,linear_word_vector, kernelX=\"Gaussian\", kernelY=\"Gaussian\")\n",
    "        basic_header = pandas.DataFrame({\n",
    "            'HSIC':pandas.Series([], dtype='float'),\n",
    "            'HSIC_LIN':pandas.Series([], dtype='float')\n",
    "        })\n",
    "        basic_header.loc['{}'.format(chunk.index[0]),'HSIC']=hsic\n",
    "        basic_header.loc['{}'.format(chunk.index[0]),'HSIC_LIN']=hsic_lin\n",
    "        if count == 0:\n",
    "            escribir_cabezote_archivo(destino, basic_header)\n",
    "        else:\n",
    "            escribir_datos_archivo(destino, basic_header)\n",
    "        count = count + 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print ('\\rDuracion:{}\\t procesados:{}\\t palabra:{}\\t'.format(elapsed_time,count, chunk.index.values[0]),end='\\t')\n",
    "    \n",
    "    return(hsic, hsic_lin)\n",
    "            \n",
    "    \n",
    "    \"\"\"print('\\n \\n Frito el pollo')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsic_no_pvalue_to_file(WORDS_ORIGIN, CITIES_ORIGIN, DESTINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:conda]",
   "language": "python",
   "name": "conda-env-conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
