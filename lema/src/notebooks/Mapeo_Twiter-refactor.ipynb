{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov  7 13:03:32 2017\n",
    "\n",
    "@author: sergio\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#import random\n",
    "import codecs\n",
    "import math\n",
    "import pandas\n",
    "from scipy.spatial import Voronoi#, voronoi_plot_2d\n",
    "from scipy.stats import wilcoxon, ranksums\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGIN = \"../static/ctc/TWITTER_COLOMBIA_RANK_NORM.csv\"\n",
    "RANK_FILE = \"../static/ctc/TWITTER_COLOMBIA_RANK.csv\"\n",
    "HSIC_FILTER = \"../static/ctc/HSIC_RECOLECT/CTC_HSIG_SOBRE_RANK_NORMALIZADO.csv\"\n",
    "COORDINATES = \"../static/geo/valid_cities_ccordinates_ext.csv\"\n",
    "\n",
    "data = pandas.read_csv(COORDINATES, sep=\"\\t\", index_col=0, decimal=b',').sort_index()\n",
    "hsic_filter_words = pandas.read_csv(HSIC_FILTER, sep=\"\\t\", index_col=0, decimal=b',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hsic_filter_words.sort_values(by=\"HSIC\", ascending=False)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isoglosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APARITION TREHS\n",
    "\n",
    "def aparition_tresh_file(file, tresh):\n",
    "    reader=pandas.read_csv(file, sep=\"\\t\", index_col=0, decimal=b',')\n",
    "    aparition = reader.iloc[0:]<(reader.iloc[0]-tresh)\n",
    "    aparition.to_csv(\"../static/ctc/aparition_{}.csv\".format(tresh), sep=\"\\t\",decimal=\",\",header=aparition.columns)\n",
    "    \n",
    "\n",
    "\n",
    "#aparition_tresh_file(RANK_FILE, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets Voronoi regions\n",
    "points = np.array(data)\n",
    "vor = Voronoi(points)\n",
    "#TOTAL_WORDS_TRESH = TOTAL_WORDS[\"#TOTAL_WORDS#\"]*APARITION_TRESH\n",
    "#VALID_WORDS_TRESH = (VALID_WORDS>TOTAL_WORDS_TRESH).astype(int)\n",
    "#VALID_WORDS_ABS_FREC =  VALID_WORDS/TOTAL_WORDS.values[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrado de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data=pandas.read_csv(ORIGIN, sep=\"\\t\", chunksize=5000, index_col=0, decimal=b',')\n",
    "\n",
    "i=0\n",
    "for chunk in source_data:\n",
    "    words_to_drop = list(set(list(chunk.index))-set(hsic_filter_words.index))\n",
    "    chunk.drop(labels=words_to_drop, inplace=True)\n",
    "    if i==0:\n",
    "        filtredwords=chunk\n",
    "    else:\n",
    "        filtredwords=filtredwords.append(chunk)\n",
    "    i+=1\n",
    "filtredwords.to_csv(\"../static/ctc/filterwords.csv\".format(i), sep=\"\\t\",decimal=\",\",header=filtredwords.columns)\n",
    "#TOTAL_WORDS = pandas.DataFrame(source_data.loc[\"#TOTAL_WORDS#\"]).drop(labels=['HSIC','PVALUE'])\n",
    "#WORDS = source_data.drop(labels=['#TOTAL_WORDS#','#TWEETS#',\"#VOCABULARY_SIZE#\",\"#USERS#\"])\n",
    "#VALID_WORDS = WORDS.loc[(WORDS['PVALUE'] < PVALUE_TRESH)].drop(labels=['HSIC','PVALUE'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_WORDS=pandas.read_csv(\"../static/ctc/filterwords.csv\", sep=\"\\t\", index_col=0, decimal=b',') \n",
    "VALID_WORDS=VALID_WORDS.transpose().sort_index().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minlength_frame(pueblo1,pueblo2):\n",
    "    A=pandas.DataFrame(VALID_WORDS[pueblo1])\n",
    "    A = A[A[pueblo1] != 1.000000e+08]\n",
    "\n",
    "    B=pandas.DataFrame(VALID_WORDS[pueblo2])\n",
    "    B = B[B[pueblo2] != 1.000000e+08]\n",
    "\n",
    "    if len(A)<len(B):\n",
    "        return (len(A), A.columns.values[0],A)\n",
    "    elif len(A)>len(B):\n",
    "        return (len(B), B.columns.values[0],B)\n",
    "    else:\n",
    "        C=pandas.concat([A,B], axis=1).fillna(1.000000e+08)\n",
    "        return (len(C), 'Iguales',C)\n",
    "\n",
    "\n",
    "def max_leng_frame(pueblo1,pueblo2):\n",
    "    A=pandas.DataFrame(VALID_WORDS[pueblo1])\n",
    "    A = A[A[pueblo1] != 1.000000e+08]\n",
    "\n",
    "    B=pandas.DataFrame(VALID_WORDS[pueblo2])\n",
    "    B = B[B[pueblo2] != 1.000000e+08]\n",
    "\n",
    "    C=pandas.concat([A,B], axis=1).fillna(1.000000e+08)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_leng_frame(pueblo1,pueblo2):\n",
    "    A=pandas.DataFrame(VALID_WORDS[pueblo1])\n",
    "    B=pandas.DataFrame(VALID_WORDS[pueblo2])\n",
    "    C=pandas.concat([A,B], axis=1)\n",
    "    return (C)\n",
    "\n",
    "def intersection_frame(pueblo1,pueblo2):\n",
    "    A=pandas.DataFrame(VALID_WORDS[pueblo1])\n",
    "    A = A[A[pueblo1] != 1.000000e+08]\n",
    "\n",
    "    B=pandas.DataFrame(VALID_WORDS[pueblo2])\n",
    "    B = B[B[pueblo2] != 1.000000e+08]\n",
    "\n",
    "    C=pandas.concat([A,B], axis=1).fillna(1.000000e+08)\n",
    "    C = C[C[pueblo2] != 1.000000e+08]\n",
    "    C = C[C[pueblo1] != 1.000000e+08]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67955801.10497236"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pueblo1=\"Medellín\"\n",
    "pueblo2=\"Santa Fe de Antioquia\"\n",
    "\n",
    "intersection_frame(pueblo1,pueblo2)[\"Medellín\"].median()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_valid_words_minlength(pueblo1,pueblo2):\n",
    "    lenx, pueblo, frame = minlength_frame(pueblo1,pueblo2)\n",
    "    ind = frame.index.values\n",
    "    if pueblo == pueblo1:\n",
    "        B=pandas.DataFrame(VALID_WORDS[pueblo2]).filter(items=ind, axis=0)\n",
    "        return(frame.join(B))\n",
    "    elif pueblo == pueblo2:\n",
    "        A=pandas.DataFrame(VALID_WORDS[pueblo1]).filter(items=ind, axis=0)\n",
    "        return(A.join(frame))\n",
    "    elif pueblo=='Iguales':\n",
    "        return(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_wilcoxon(pueblo1, pueblo2):\n",
    "    #vectores=join_valid_words_minlength(pueblo1,pueblo2)\n",
    "    vectores=max_leng_frame(pueblo1,pueblo2)\n",
    "    #vectores=intersection_frame(pueblo1,pueblo2)\n",
    "    n = len(vectores)\n",
    "    s=n*(n+1)/2\n",
    "    maxim = max(vectores[pueblo1].max(),vectores[pueblo2].max())\n",
    "    minim = min(vectores[pueblo1].min(),vectores[pueblo2].min())\n",
    "    diferencia = abs((vectores[pueblo1].rank(method=\"dense\", ascending=False)-vectores[pueblo2].rank(method=\"dense\", ascending=False)).sum())\n",
    "    wil, pvalue = wilcoxon(vectores[pueblo1],vectores[pueblo2])\n",
    "    peso = diferencia/(n*(maxim-minim))\n",
    "    return(peso, wil, pvalue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def constante_proporcionalidad(pueblo1, pueblo2):\\n    corpus=VALID_WORDS\\n    commun_corpus = max_leng_frame(pueblo1,pueblo2)\\n    commun_says =  commun_corpus[commun_corpus[pueblo1] != 1.000000e+08]\\n    commun_says =  commun_says[commun_says[pueblo2] != 1.000000e+08]\\n    not_in_smaller = len(commun_corpus.index)-len(commun_says)\\n    ausentes_corpus=len(corpus.index)-len(commun_corpus.index)\\n    k=(len(commun_says.index)+ausentes_corpus)*len(commun_corpus.index)/(((len(commun_says.index)+not_in_smaller)*(len(corpus.index))))\\n    return  k'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def constante_proporcionalidad(pueblo1, pueblo2):\n",
    "    corpus=VALID_WORDS\n",
    "    commun_corpus = max_leng_frame(pueblo1,pueblo2)\n",
    "    commun_says =  commun_corpus[commun_corpus[pueblo1] != 1.000000e+08]\n",
    "    commun_says =  commun_says[commun_says[pueblo2] != 1.000000e+08]\n",
    "    not_in_smaller = len(commun_corpus.index)-len(commun_says)\n",
    "    ausentes_corpus=len(corpus.index)-len(commun_corpus.index)\n",
    "    k=(len(commun_says.index)+ausentes_corpus)*len(commun_corpus.index)/(((len(commun_says.index)+not_in_smaller)*(len(corpus.index))))\n",
    "    return  k'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigth_values():\n",
    "    weigth_values = {}    \n",
    "    for i in range(len(vor.ridge_vertices)):\n",
    "        vertex1,vertex2=vor.ridge_vertices[i]  # points in the map defining the line\n",
    "        city1,city2=vor.ridge_points[i]  # indexes of the cities separated by the line   \n",
    "        if vertex1==-1 or vertex2==-1:\n",
    "            continue\n",
    "        pueblo1=VALID_WORDS.iloc[:,city1].name\n",
    "        pueblo2=VALID_WORDS.iloc[:,city2].name\n",
    "        frontera = '{}--{}'.format(pueblo1,pueblo2)\n",
    "        peso, wil, pvalue=this_wilcoxon(pueblo1,pueblo2)\n",
    "       # peso = peso * constante_proporcionalidad(pueblo1, pueblo2) \n",
    "        if (pvalue>0.05):\n",
    "            color=(0,1,1,1)\n",
    "        if (pvalue<=0.05):\n",
    "            color=(1,0,0,1)\n",
    "        weigth_values[frontera]={'peso':peso,'color':color, 'orden':i}\n",
    "    weigth_values=pandas.DataFrame.from_dict(weigth_values, orient='index').sort_values(by=['peso'],  ascending=False)\n",
    "    maxi = weigth_values['peso'].max()\n",
    "    mini = weigth_values['peso'].min()\n",
    "    return (maxi, mini, weigth_values)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/scipy/stats/morestats.py:2388: UserWarning: Warning: sample size too small for normal approximation.\n",
      "  warnings.warn(\"Warning: sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=0.5, pvalue=0.059058229090536707)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=[1,2,3,4,5]\n",
    "B=[7,9,3.3,9,5]\n",
    "wilcoxon(A,B,zero_method=\"zsplit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maping():\n",
    "    plt.close('all')\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(80,40))\n",
    "\n",
    "    # setup Lambert Conformal basemap.\n",
    "    m = Basemap(width=1500000,height=2000000,projection='lcc',\n",
    "            resolution='h',lat_1=15.,lat_2=25,lat_0=3.9,lon_0=-73.)\n",
    "    # draw coastlines.\n",
    "    m.drawcoastlines(linewidth=4)\n",
    "    m.drawcountries(linewidth=4)\n",
    "    #m.drawmapboundary(fill_color='aquamarine')\n",
    "    #m.fillcontinents(color=\"ivory\",lake_color=\"aquamarine\")\n",
    "    #m.etopo()\n",
    "    m.drawstates()\n",
    "    #m.shadedrelief()\n",
    "\n",
    "    # draws locations\n",
    "    for index, row in data.iterrows():\n",
    "        m.tissot(row['Longitud'],row['Latitud'],radius_deg=5/200,npts=20)\n",
    "        x,y=m(row['Longitud'],row['Latitud'])\n",
    "        plt.text(x,y,index,fontsize=7)\n",
    "   \n",
    "    \n",
    "    maxi,mini,weigth = weigth_values()\n",
    "    weigth.reset_index(level=0, inplace=True)\n",
    "    weigth.set_index('orden', inplace=True)\n",
    "    weigth.sort_index(inplace=True)\n",
    "    for i in range(len(vor.ridge_vertices)):\n",
    "        vertex1,vertex2=vor.ridge_vertices[i]  # points in the map defining the line\n",
    "        city1,city2=vor.ridge_points[i]  # indexes of the cities separated by the line\n",
    "        if vertex1==-1 or vertex2==-1:\n",
    "            continue\n",
    "        line_weight=(weigth.loc[i][\"peso\"]-mini)/(maxi-mini)\n",
    "        latitude1,longitude1=vor.vertices[vertex1]\n",
    "        latitude2,longitude2=vor.vertices[vertex2]\n",
    "        x,y=m([longitude1,longitude2],[latitude1,latitude2])    \n",
    "        m.plot(x,y,color=weigth.loc[i][\"color\"],linewidth=10*(line_weight))\n",
    "    plt.savefig('../static/ctc/prv_1.png', format='png', dpi=200)\n",
    "    plt.clf()\n",
    "maping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:conda]",
   "language": "python",
   "name": "conda-env-conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
