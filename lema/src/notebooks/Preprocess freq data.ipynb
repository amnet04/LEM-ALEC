{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constants definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders\n",
    "ORIGINAL_DATA_FOLDER=\"/src/data/originales/\"\n",
    "PROCECED_DATA_FOLDER=\"/src/data/procesados/\"\n",
    "\n",
    "# original data files\n",
    "ORIGINAL_FREQ_SOURCE = \"{}TWITTER_COLOMBIA_FREQ.csv\".format(ORIGINAL_DATA_FOLDER)\n",
    "ORIGINAL_CITIES_SOURCE = \"{}cities_coordinates.csv\".format(ORIGINAL_DATA_FOLDER)\n",
    "\n",
    "# Proceced data files\n",
    "CLEAN_FREQ_FILE = \"{}clean/TWITTER_COLOMBIA_FREQ_CLEAN.csv\".format(PROCECED_DATA_FOLDER)\n",
    "REL_FREQ_DATA_DES = \"{}clean/TWITTER_COLOMBIA_RELATIVE_FREQ_CLEAN.csv\".format(PROCECED_DATA_FOLDER)\n",
    "RANK_DATA_DES = \"{}clean/TWITTER_COLOMBIA_RANK.csv\".format(PROCECED_DATA_FOLDER)\n",
    "NOR_RANK_DES = \"{}clean/TWITTER_COLOMBIA_RANK_NORM.csv\".format(PROCECED_DATA_FOLDER)\n",
    "FILTER_CITIES = \"{}/geo/filter_cities_coordinates.csv\".format(PROCECED_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_dataframe_to_csv_write(dataframe, destino):\n",
    "    dataframe.to_csv(\n",
    "        destino, \n",
    "        sep=\"\\t\",\n",
    "        decimal=\",\",\n",
    "        encoding='utf-8',\n",
    "        header=dataframe.columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidWordsFrame(file):\n",
    "    general_words_data = pandas.read_csv(\n",
    "        file, \n",
    "        encoding='utf-8', \n",
    "        usecols=[*range(0,6)],\n",
    "        skiprows=[*range(1,6)],\n",
    "        dtype={'total':np.int32,'lower':np.int32, 'titled':np.int32, 'upper':np.int32, 'other':np.int32},\n",
    "        index_col = ['#CITY#']\n",
    "    )\n",
    "    \n",
    "    totales = {}\n",
    "    total= len(general_words_data.index)\n",
    "    \n",
    "    #filtrar las palabras con una sola aparición, porque dan hsic malos\n",
    "    valid_words=general_words_data.loc[general_words_data['total']>1]\n",
    "    one_app = total - len(valid_words)\n",
    "    \n",
    "    #filtrar acronimos\n",
    "    valid_words=valid_words.loc[valid_words['lower']>valid_words['upper']]\n",
    "    acronims = total+ one_app - len(valid_words)\n",
    "    \n",
    "    #filtrar posibles nombres propios\n",
    "    valid_words=valid_words.loc[valid_words['lower']>valid_words['titled']]\n",
    "    personal_names = total+one_app+acronims-len(valid_words)\n",
    "    \n",
    "    #obtener palabras válidas hasta aquí\n",
    "    valid_words = pandas.DataFrame(valid_words.index)\n",
    "    \n",
    "    \n",
    "    # Filtrar otras cosas\n",
    "    \n",
    "    # Filtro de posibles links\n",
    "    http_pat = re.compile(r'.*http.*',flags=re.IGNORECASE)\n",
    "    valid_words['http'] = valid_words['#CITY#'].str.contains(http_pat)\n",
    "    valid_words=valid_words.loc[valid_words['http']==False]\n",
    "    links = total+one_app+acronims+personal_names-len(valid_words)\n",
    "\n",
    "    # Filtro risas \n",
    "    risa_pat = re.compile(r'.*(ja|je|ji|jo|ha|he|hi|ho){2,}.*',flags=re.IGNORECASE)\n",
    "    valid_words['risa'] = valid_words['#CITY#'].str.extract(risa_pat, expand=True)\n",
    "    valid_words['risa'] = ~valid_words['risa'].isnull()\n",
    "    valid_words=valid_words.loc[valid_words['risa']==False]\n",
    "    laugths = total+one_app+acronims+personal_names+ links-len(valid_words)\n",
    "    \n",
    "    # Filtro pics al final, porque no se que es pero me late que es algo de twiter, una foto posiblemente\n",
    "    pic_pat = re.compile(r'.*pic$',flags=re.IGNORECASE)\n",
    "    valid_words['pic'] = valid_words['#CITY#'].str.contains(pic_pat)\n",
    "    valid_words=valid_words.loc[valid_words['pic']==False]\n",
    "    pics = total+one_app+acronims+personal_names+ links+ laugths -len(valid_words)\n",
    "    \n",
    "    # Filtro palabras con mucha repetición de letras (más de tres letras iguales repetidas)\n",
    "    multiple_path = re.compile(r'(\\w)\\1{2,}',flags=re.IGNORECASE)\n",
    "    valid_words['multiple'] = valid_words['#CITY#'].str.extract(multiple_path, expand=True)\n",
    "    valid_words['multiple'] = ~valid_words['multiple'].isnull()    \n",
    "    valid_words=valid_words.loc[valid_words['multiple']==False]\n",
    "    multiple = total+one_app+acronims+personal_names+ links+ laugths + pics -len(valid_words)\n",
    "    \n",
    "    # Filtro palabras con numeros\n",
    "    numeros_path = re.compile(r'.*(\\d).*',flags=re.IGNORECASE)\n",
    "    valid_words['numeros'] = valid_words['#CITY#'].str.extract(multiple_path, expand=True)\n",
    "    valid_words['numeros'] = ~valid_words['numeros'].isnull()\n",
    "    valid_words=valid_words.loc[valid_words['numeros']==False]\n",
    "    valid_words=valid_words.set_index('#CITY#')\n",
    "    with_numbers = total+one_app+acronims+personal_names+ links+ laugths + pics + multiple -len(valid_words)\n",
    "    \n",
    "    filter_sum = one_app+acronims+personal_names+ links+ laugths + pics + multiple + with_numbers\n",
    "    filter_len = len(valid_words)\n",
    "    \n",
    "    return(valid_words.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidWords=ValidWordsFrame(ORIGINAL_FREQ_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cities filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_data(file):\n",
    "    general_cities_data =pandas.read_csv(\n",
    "        file, \n",
    "        encoding='utf-8',\n",
    "        nrows=5,\n",
    "        index_col = ['#CITY#']\n",
    "    )\n",
    "    general_cities_data.drop(labels=['total','lower', 'titled', 'upper','other'], axis=1, inplace=True)\n",
    "    general_cities_data.drop(labels=['#COUNTRY#','#TWEETS#', '#USERS#'], axis=0, inplace=True)\n",
    "    general_cities_data=general_cities_data.transpose()\n",
    "    max_words = general_cities_data['#TOTAL_WORDS#'].max()\n",
    "    max_vocabulary_size = general_cities_data['#VOCABULARY_SIZE#'].max()\n",
    "    return(max_words, max_vocabulary_size, general_cities_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_w, max_v, Vocabulary_Data = vocabulary_data(ORIGINAL_FREQ_SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterCitiesBy(data, order_by=False, word_tresh=0, vocabulary_tresh=0):\n",
    "    localidades_filtradas = data\n",
    "    localidades_filtradas['#TOTAL_WORDS#']=localidades_filtradas['#TOTAL_WORDS#'].apply(pandas.to_numeric)\n",
    "    localidades_filtradas['#VOCABULARY_SIZE#']=localidades_filtradas['#VOCABULARY_SIZE#'].apply(pandas.to_numeric)\n",
    "    if word_tresh != 0:\n",
    "        localidades_filtradas = localidades_filtradas.loc[localidades_filtradas[\"#TOTAL_WORDS#\"]>=word_tresh]\n",
    "    if vocabulary_tresh != 0:\n",
    "        localidades_filtradas = localidades_filtradas.loc[localidades_filtradas[\"#VOCABULARY_SIZE#\"]>=vocabulary_tresh]\n",
    "    if order_by:\n",
    "        localidades_filtradas = localidades_filtradas.sort_values(by=order_by)\n",
    "    else:\n",
    "        localidades_filtradas = localidades_filtradas\n",
    "    return(localidades_filtradas.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidCities = FilterCitiesBy(Vocabulary_Data, order_by=False, word_tresh=50000, vocabulary_tresh=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_cities_coordinates(origin, destination, ValidCities):\n",
    "    all_cities_coordinates =pandas.read_csv(\n",
    "        origin, \n",
    "        encoding='utf-8',\n",
    "        sep=\"\\t\",\n",
    "        decimal=\",\",\n",
    "        index_col = ['ciudad']\n",
    "    )\n",
    "    cities_to_drop = list(set(list(all_cities_coordinates.index))-set(ValidCities))\n",
    "    valid_cities = all_cities_coordinates.drop(labels=cities_to_drop)\n",
    "    regular_dataframe_to_csv_write(valid_cities, destination)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Apply words an cities filters to all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FiltredWordLocationFrame(origin, destination, ValidCities, ValidWords ):\n",
    "    lenvalidcities=len(ValidCities)\n",
    "    \n",
    "    file_word_freq = pandas.read_csv(\n",
    "        origin, \n",
    "        encoding='utf-8',\n",
    "        chunksize=10000, \n",
    "        index_col=0,\n",
    "        skiprows=[1,3,5],\n",
    "        sep =\",\", \n",
    "        quotechar='\"'\n",
    "    )\n",
    "    \n",
    "    i=0\n",
    "    for chunk in file_word_freq:\n",
    "        cities_to_drop = list(set(list(chunk.columns))-set(ValidCities))\n",
    "        words_to_drop = list(set(list(chunk.index))-set(ValidWords))\n",
    "        chunk.drop(labels=cities_to_drop, axis=1, inplace=True)\n",
    "        chunk.drop(labels=words_to_drop, inplace=True)\n",
    "        if i==0:\n",
    "            filtredwordsfreq=chunk\n",
    "        else:\n",
    "            filtredwordsfreq=filtredwordsfreq.append(chunk)\n",
    "        print('\\rLoop {}. Borrados = {}, tamaño actual del arreglo: {}'.format(i, len(words_to_drop), len(filtredwordsfreq)), end=\"\\t\\t\\t\")\n",
    "        i+=1\n",
    "    totales=pandas.DataFrame(filtredwordsfreq.sum(),columns=['#TOTAL_WORDS#']).transpose()\n",
    "    filtredwordsfreq = pandas.concat([totales, filtredwordsfreq])    \n",
    "            \n",
    "    regular_dataframe_to_csv_write(filtredwordsfreq, destination)\n",
    "        \n",
    "    print('\\r Listo!!', end=\"\\t\\t\\t\\t\\t\\t\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_freq_file(file,destino):\n",
    "    file_word_freq = pandas.read_csv(\n",
    "        file, \n",
    "        encoding='utf-8', \n",
    "        index_col=0,\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"'\n",
    "    )\n",
    "    frecuencias_relativas = file_word_freq.iloc[0:]/file_word_freq.iloc[0]\n",
    "    frecuencias_relativas.to_csv(destino, sep=\"\\t\",decimal=\",\",header=frecuencias_relativas.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_file(file,destino):\n",
    "    file_word_rank = pandas.read_csv(\n",
    "        file, \n",
    "        encoding='utf-8', \n",
    "        index_col=0,\n",
    "        skiprows=[1],\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"'\n",
    "    )\n",
    "    file_word_rank = file_word_rank.rank(method='dense',ascending=False)\n",
    "    max_rank = file_word_rank.max(axis=0)\n",
    "    file_word_rank = pandas.concat([pandas.DataFrame(max_rank, columns=['#MAX_RANK#']).transpose(), file_word_rank])    \n",
    "    file_word_rank = file_word_rank.astype(int)\n",
    "    file_word_rank.to_csv(destino, sep=\"\\t\",decimal=\",\",header=file_word_rank.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliced_rank_file(file,destino):\n",
    "    normaliced_word_rank = pandas.read_csv(\n",
    "        file, \n",
    "        encoding='utf-8', \n",
    "        index_col=0,\n",
    "        sep =\"\\t\",\n",
    "        decimal=\",\",\n",
    "        quotechar='\"'\n",
    "    )\n",
    "    normaliced_word_rank = (normaliced_word_rank.iloc[0:]/normaliced_word_rank.iloc[0])*100000000\n",
    "    normaliced_word_rank.to_csv(destino, sep=\"\\t\",decimal=\",\",header=normaliced_word_rank.columns)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:conda]",
   "language": "python",
   "name": "conda-env-conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
